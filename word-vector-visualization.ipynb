{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Word2vec tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "import gensim\n",
    "import gensim.downloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get the interactive tools for matplotlib\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import numpy as np\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim isn't really a deep learning package. It's a package for for word and text similarity modeling, which started with (LDA-style) topic models and grew into SVD and neural word representations. But its efficient and scalable, and quite widely used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "Word2Vec is a popular algorithm for learning word embeddings, which are dense vector representations of words in a continuous space. These embeddings capture semantic relationships between words and enable various natural language processing tasks. \"word2vec-google-news-300\" specifically refers to a pre-trained Word2Vec model developed by Google, trained on a vast corpus of news articles.\n",
    "\n",
    "Here are some key characteristics and points about the \"word2vec-google-news-300\" model:\n",
    "\n",
    "1- 300-Dimensional Vectors: The model generates word embeddings in a 300-dimensional vector space. Each dimension of the vector captures a specific aspect of the word's meaning or context.\n",
    "\n",
    "2- Semantic Similarity: The model's embeddings are known for capturing semantic relationships between words. Words that are similar in meaning tend to have vectors that are close together in the vector space.\n",
    "\n",
    "3- Pre-trained on News Corpus: This model is trained on a massive collection of news articles, which means it is particularly good at capturing language related to current events, news topics, and general vocabulary.\n",
    "\n",
    "4- Generalization: The model's embeddings can be used as features for various NLP tasks, such as sentiment analysis, text classification, and named entity recognition. Its ability to generalize from news articles can make it useful for a wide range of applications.\n",
    "\n",
    "5- Out-of-Vocabulary Words: While the model can provide embeddings for a vast vocabulary, it might not have embeddings for very rare or specialized words, which could limit its utility in certain domains.\n",
    "\n",
    "6- Word Analogies: Word2Vec embeddings often exhibit interesting properties, such as the ability to perform word analogies like \"king - man + woman = queen.\" This is due to the vector space capturing relationships between words.\n",
    "\n",
    "7- Large-Scale Training Data: The quality of the embeddings is partly attributed to the vast amount of news data used for training. This helps the model learn rich and nuanced representations of words.\n",
    "\n",
    "8- Resource Intensive: The vector embeddings are real-valued numbers, resulting in a relatively high memory requirement to store the model and perform computations. However, these vectors can be loaded into memory to efficiently perform similarity calculations.\n",
    "\n",
    "9- Pre-trained Model Availability: The \"word2vec-google-news-300\" model is publicly available and can be downloaded from various sources. It can be used as a starting point for various NLP projects, saving time and computational resources that would be required for training from scratch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.get_vector(\"car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Embedding vector size is {}\".format(model.get_vector(\"car\").size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most similar to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.most_similar('obama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.most_similar('banana')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = model.most_similar(positive=['woman','king'], negative=['man'])\n",
    "print(\"{}:{:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analogy(x1,x2,y1):\n",
    "    result = model.most_similar(positive=[y1,x2], negative=[x1])\n",
    "    return result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analogy('france','paris','spain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analogy('japan','japanese','germany')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analogy('japan','japanese','sweden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analogy('japan','japanese','canada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analogy('ireland','bread','france')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analogy('tall','tallest','long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analogy('good','fantastic','bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analogy('good','best','bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the intruder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model.doesnt_match(\"car wheel driver bread\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_pca_scatterplot(model,words=None,sample=0):\n",
    "    if words == None:\n",
    "        if sample > 0:\n",
    "            words = np.random.choice(list(model.vocab.keys()),sample)\n",
    "        else:\n",
    "            words = [word for word in model.vocab]\n",
    "    \n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "words = ['coffee', 'wednesday', 'monday', 'friday' , 'sunday', 'tuesday' ,'tea', 'beer', 'wine', 'brandy', 'rum', 'champagne', 'water',\n",
    "                         'spaghetti', 'borscht', 'hamburger', 'pizza', 'falafel', 'sushi', 'meatballs',\n",
    "                         'dog', 'horse', 'cat', 'monkey', 'parrot', 'koala', 'lizard', 'italy','uniform','spain',\n",
    "                         'frog', 'toad', 'monkey', 'ape', 'kangaroo', 'wombat', 'wolf',\n",
    "                         'france', 'germany', 'donkey', 'australia', 'water',\n",
    "                         'homework', 'assignment', 'problem', 'exam', 'test', 'class',\n",
    "                         'school', 'college', 'university', 'institute', 'six', 'two', 'three','four','five']\n",
    "\n",
    "display_pca_scatterplot(model, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if words == None:\n",
    "        if sample > 0:\n",
    "            words = np.random.choice(list(model.vocab.keys()),sample)\n",
    "        else:\n",
    "            words = [word for word in model.vocab]\n",
    "    \n",
    "word_vectors = np.array([model[w] for w in words])\n",
    "embeddings_3d = PCA(n_components=3).fit_transform(word_vectors)\n",
    "    \n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=embeddings_3d[:, 0],\n",
    "    y=embeddings_3d[:, 1],\n",
    "    z=embeddings_3d[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=3,\n",
    "        opacity=0.5\n",
    "    ),\n",
    "    text=words,\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "# Set plot title and axis labels\n",
    "fig.update_layout(\n",
    "    title='Word2Vec Word Embeddings (3D)',\n",
    "    scene=dict(\n",
    "        xaxis_title='Dimension 1',\n",
    "        yaxis_title='Dimension 2',\n",
    "        zaxis_title='Dimension 3'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=1000,\n",
    "    paper_bgcolor=\"LightSteelBlue\",\n",
    ")\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
